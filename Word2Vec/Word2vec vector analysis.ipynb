{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI630 Homework 2: Word2vec Vector Analysis\n",
    "\n",
    "*Important Note:* Start this notebook only after you've gotten your word2vec model up and running!\n",
    "\n",
    "Many NLP packages support working with word embeddings. In this notebook you can work through the various problems assigned in Task 3. We've provided the basic functionality for loading word vectors using [Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html), a good library for learning and using word vectors, and for working with the vectors. \n",
    "\n",
    "One of the fun parts of word vectors is getting a sense of what they learned. Feel free to explore the vectors here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username: hexinz\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('output.wv', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wernerian', 0.31261146068573),\n",
       " ('Gynaecologists', 0.2848040461540222),\n",
       " ('REG1', 0.27965983748435974),\n",
       " ('Mordialloc', 0.27877068519592285),\n",
       " ('Juried', 0.2782945930957794),\n",
       " ('Mardhiah', 0.27755188941955566),\n",
       " ('Phitthaya', 0.2770390808582306),\n",
       " ('Frauenbund', 0.27112919092178345),\n",
       " ('Cremorne', 0.2681736946105957),\n",
       " ('Chríost', 0.26729196310043335)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_vectors['the']\n",
    "word_vectors.similar_by_word(\"and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.9444119334220886),\n",
       " ('grandfather', 0.9349886775016785),\n",
       " ('eldest', 0.9347566962242126),\n",
       " ('grandson', 0.9312842488288879),\n",
       " ('child', 0.9305014610290527),\n",
       " ('heir', 0.9261252880096436),\n",
       " ('sons', 0.9237461090087891),\n",
       " ('younger', 0.9226715564727783),\n",
       " ('brothers', 0.9212406873703003),\n",
       " ('elder', 0.9160992503166199)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"son\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Born', 0.9310526847839355),\n",
       " ('raised', 0.9041904807090759),\n",
       " ('2019', 0.8900091052055359),\n",
       " ('neighborhood', 0.8882691860198975),\n",
       " ('grew', 0.8880464434623718),\n",
       " ('Studios', 0.8847413659095764),\n",
       " ('wealthy', 0.8807938694953918),\n",
       " ('Education', 0.8804402351379395),\n",
       " ('Long', 0.8804317712783813),\n",
       " ('August', 0.8778650760650635)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"born\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goalscorer', 0.9492651224136353),\n",
       " ('Top', 0.9429421424865723),\n",
       " ('scorer', 0.9418091177940369),\n",
       " ('scorers', 0.9362465143203735),\n",
       " ('highest', 0.9314957857131958),\n",
       " ('rotation', 0.9287024140357971),\n",
       " ('spot', 0.9262141585350037),\n",
       " ('grossing', 0.9254440069198608),\n",
       " ('ranking', 0.9253345727920532),\n",
       " ('relegation', 0.9249082803726196)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occasional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('successfully', 0.9462027549743652),\n",
       " ('finally', 0.9460392594337463),\n",
       " ('however', 0.9392870664596558),\n",
       " ('McHale', 0.9370847940444946),\n",
       " ('therefore', 0.9323566555976868),\n",
       " ('profession', 0.9318313598632812),\n",
       " ('Serena', 0.9316397905349731),\n",
       " ('thus', 0.9305494427680969),\n",
       " ('tag', 0.930137038230896),\n",
       " ('formation', 0.9297938346862793)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"eventually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exciting', 0.9361180067062378),\n",
       " ('terrible', 0.9331440329551697),\n",
       " ('fantastic', 0.9317662715911865),\n",
       " ('mean', 0.9301401376724243),\n",
       " ('replied', 0.9280708432197571),\n",
       " ('happens', 0.9274816513061523),\n",
       " ('What', 0.9268141388893127),\n",
       " ('Melody', 0.9259214401245117),\n",
       " ('remarkable', 0.9257221221923828),\n",
       " ('unlikely', 0.9256329536437988)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Korea', 0.9443014860153198),\n",
       " ('Taiwan', 0.932518482208252),\n",
       " ('Beijing', 0.92923903465271),\n",
       " ('Kenya', 0.9283241033554077),\n",
       " ('Republic', 0.9253071546554565),\n",
       " ('Turkey', 0.9252933859825134),\n",
       " ('Hong', 0.9238981008529663),\n",
       " ('Malaysia', 0.9235707521438599),\n",
       " ('documentary', 0.921799898147583),\n",
       " ('Italy', 0.9209052324295044)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nervous', 0.9037822484970093),\n",
       " ('privacy', 0.8916728496551514),\n",
       " ('detailing', 0.8879326581954956),\n",
       " ('sentencing', 0.886925995349884),\n",
       " ('opium', 0.8844797611236572),\n",
       " ('disasters', 0.8843635320663452),\n",
       " ('protracted', 0.8824070692062378),\n",
       " ('blindness', 0.8817066550254822),\n",
       " ('intense', 0.881163477897644),\n",
       " ('interrogation', 0.880934476852417)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"SARS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('babies', 0.9478209018707275),\n",
       " ('journalists', 0.9245781898498535),\n",
       " ('deaths', 0.920008659362793),\n",
       " ('attendants', 0.9135136008262634),\n",
       " ('targets', 0.9130617380142212),\n",
       " ('homosexuals', 0.9128782749176025),\n",
       " ('inmates', 0.9089660048484802),\n",
       " ('unconfirmed', 0.9085569381713867),\n",
       " ('creditors', 0.9084674715995789),\n",
       " ('rumored', 0.9084410667419434)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"captives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('everyone', 0.9415018558502197),\n",
       " ('pretty', 0.939758837223053),\n",
       " ('wisdom', 0.93647301197052),\n",
       " ('knowing', 0.935626745223999),\n",
       " ('contradiction', 0.9352973103523254),\n",
       " ('laughing', 0.9343644380569458),\n",
       " ('joke', 0.9340001344680786),\n",
       " ('know', 0.932907223701477),\n",
       " ('honest', 0.9300961494445801),\n",
       " ('skeptical', 0.9278777241706848)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"stupid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Firstly, we select four common words: 'and', 'son', 'born', and 'top'. 'and' and 'son' appear more than 100,000 times; 'born' and 'top' appear more than 10,000 times. For 'and', the predicted similar words are rare words; for 'son', 'born' and 'top', the predicted similar words are reasonable: more verbs and words related to time appear in the similar words of 'born' and more words related to identity appear in the similar words of 'son'.\n",
    "- Secondly, we select four occasional words: 'eventually', 'amazing', 'China' and 'SARS'. They appear more than 100 and less than 10,000 times. Similar words are successfully predicted. \n",
    "- Finally, we select two rare words: 'captives' and , which appear less than 100 times. Similar words are partially successfully predicted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(a, b, c):\n",
    "    return word_vectors.most_similar(positive=[b, c], negative=[a])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sister'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('son', 'daughter', 'elder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exhausting'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('student', 'teacher', 'homework')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'older'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('young', 'old', 'younger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'producer'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('novel', 'movie', 'writer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'girl'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('short', 'long', 'young')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, among five selected word analies, four of them have good performance.\n",
    "- daughter + elder - son = sister, \n",
    "- student - homework = teacher - exhausting\n",
    "- young - younger = old - older\n",
    "- novel - writer = movie - producer\n",
    "\n",
    "However, we could not get 'old' from 'long + young - short', indicating that the word2vec vector works better on synonymity than antonymy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_syn = KeyedVectors.load_word2vec_format('output_update.wv', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('word_pairs_to_estimate_similarity.test.csv')\n",
    "id = data['pair_id'].values\n",
    "word1 = data['word1'].values\n",
    "word2 = data['word2'].values\n",
    "sim = []\n",
    "for i in id:\n",
    "    sim.append(word_vectors.similarity(word1[i], word2[i]))\n",
    "result = {'id': id, 'sim': sim}\n",
    "result = pd.DataFrame(result)\n",
    "header = ['pair_id', 'similarity']\n",
    "result.to_csv('result.csv', header=header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('word_pairs_to_estimate_similarity.test.csv')\n",
    "id = data['pair_id'].values\n",
    "word1 = data['word1'].values\n",
    "word2 = data['word2'].values\n",
    "sim = []\n",
    "for i in id:\n",
    "    sim.append(word_vectors_syn.similarity(word1[i], word2[i]))\n",
    "result = {'id': id, 'sim': sim}\n",
    "result = pd.DataFrame(result)\n",
    "header = ['pair_id', 'similarity']\n",
    "result.to_csv('result2.csv', header=header, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('summit', 0.9020704627037048),\n",
       " ('meridian', 0.8762383460998535),\n",
       " ('pinnacle', 0.8365438580513),\n",
       " ('superlative', 0.8226457834243774),\n",
       " ('elevation', 0.8080981969833374),\n",
       " ('height', 0.8065374493598938),\n",
       " ('peak', 0.8059883713722229),\n",
       " ('comebacks', 0.7384368181228638),\n",
       " ('Influencers', 0.7248468995094299),\n",
       " ('Sandanme', 0.7173957228660583)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_syn.similar_by_word(\"top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ultimately', 0.7511016130447388),\n",
       " ('Vyronas', 0.6867636442184448),\n",
       " ('Garachiné', 0.6825960278511047),\n",
       " ('Afterward', 0.6714228391647339),\n",
       " ('Oaș', 0.6677548289299011),\n",
       " ('initially', 0.6623428463935852),\n",
       " ('Barlog', 0.6619102954864502),\n",
       " ('luxuriously', 0.6592634916305542),\n",
       " ('physic', 0.6558774709701538),\n",
       " ('Duerrstein', 0.6493158936500549)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_syn.similar_by_word(\"eventually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awesome', 0.8943012952804565),\n",
       " ('awful', 0.8396655321121216),\n",
       " ('unbelievable', 0.8320059776306152),\n",
       " ('wondrous', 0.820950984954834),\n",
       " ('unbelievably', 0.8129263520240784),\n",
       " ('pleasurable', 0.8038662672042847),\n",
       " ('marvelous', 0.8005156517028809),\n",
       " ('cheeky', 0.7975283861160278),\n",
       " ('incredible', 0.7962194085121155),\n",
       " ('crap', 0.7933977246284485)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_syn.similar_by_word(\"amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fetch', 0.6756247282028198),\n",
       " ('convince', 0.6570402383804321),\n",
       " ('impart', 0.6450479030609131),\n",
       " ('modernize', 0.6416994333267212),\n",
       " ('Vassouras', 0.6409450173377991),\n",
       " ('avail', 0.6392044425010681),\n",
       " ('maintain', 0.6346015930175781),\n",
       " ('encourage', 0.6322163343429565),\n",
       " ('MercyMe', 0.630827009677887),\n",
       " ('cater', 0.6286659240722656)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_syn.similar_by_word(\"give\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('giving', 0.9432541131973267),\n",
       " ('access', 0.9427363276481628),\n",
       " ('bring', 0.9392701387405396),\n",
       " ('drop', 0.9363373517990112),\n",
       " ('offer', 0.9341126680374146),\n",
       " ('favour', 0.9329581260681152),\n",
       " ('keep', 0.9326536655426025),\n",
       " ('see', 0.9322749376296997),\n",
       " ('hear', 0.9315024018287659),\n",
       " ('gives', 0.9314975738525391)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"give\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('formation', 0.7976463437080383),\n",
       " ('establishment', 0.7773997783660889),\n",
       " ('realignment', 0.7506915330886841),\n",
       " ('integral', 0.7382983565330505),\n",
       " ('reorganisation', 0.7266575694084167),\n",
       " ('constitutional', 0.7212973833084106),\n",
       " ('CPNC', 0.7196764945983887),\n",
       " ('feinting', 0.7195417881011963),\n",
       " ('stakeholder', 0.7175673246383667),\n",
       " ('rationalization', 0.7112507224082947)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_syn.similar_by_word(\"constitution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laws', 0.9439664483070374),\n",
       " ('evidence', 0.9363945722579956),\n",
       " ('corruption', 0.9318532347679138),\n",
       " ('conclusions', 0.9308825731277466),\n",
       " ('constitutional', 0.9297701120376587),\n",
       " ('yet', 0.9284285306930542),\n",
       " ('changes', 0.9282674193382263),\n",
       " ('existence', 0.928173840045929),\n",
       " ('values', 0.9261394143104553),\n",
       " ('phase', 0.9260390400886536)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"constitution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select five words: son, top, eventually, SARS and stupid\n",
    "We can see that for adjectives and adverbs, the synonym-aware model has better performance since they contain more synonyms. For example:\n",
    "- the synonym-aware model for 'eventually' has 'ultimately'. On the contrary, the non-synonym-aware model for 'eventually' only has 'successfully'.\n",
    "- the synonym-aware model for 'top' has 'peak', 'summit', 'elecation', 'superlative' and so on while the non-synonym-aware model for 'top' don't.\n",
    "- the synonym-aware model for 'amazing' has 'awesome', 'awful', 'unbelievable' and so on while the non-synonym-aware model for 'amazing' don't.\n",
    "- the synonym-aware model for 'give' has 'fetch', 'impart', 'MercyMe' and so on while the non-synonym-aware model for 'give' has 'giving', 'offer', 'gives'. They have similar performance.\n",
    "- the synonym-aware model for 'constitution' has 'constitutional', 'formation', 'establishment' and so on while the non-synonym-aware model for 'constitution' don't.\n",
    "Therefore, from my perspective, the synonym-aware model does produce better vectors than the non-synonym-aware model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
